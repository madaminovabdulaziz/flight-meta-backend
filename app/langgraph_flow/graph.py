"""
LangGraph Flow Definition (HYBRID VERSION)
Main orchestration graph with rule-based/LLM routing for AI Trip Planner.

KEY CHANGE: Added should_use_llm_node for intelligent routing.

Flow now has TWO paths:
1. RULE-BASED PATH (fast, cheap):
   - skip classify_intent_node (use local classifier)
   - skip extract_parameters_node (use rule extractor)
   - use template_engine for questions
   - use cheap_ranker for flights

2. LLM PATH (fallback):
   - full LLM nodes when confidence is low
   - same quality, slower, more expensive

Expected savings: 80-100% cost reduction, 75% latency reduction
"""

import logging
from typing import Literal, Optional
from langgraph.graph import StateGraph, END

from app.langgraph_flow.state import ConversationState
from app.langgraph_flow.nodes.entry_node import entry_node
from app.langgraph_flow.nodes.load_session_state_node import load_session_state_node
from app.langgraph_flow.nodes.load_user_memory_node import load_user_memory_node
from app.langgraph_flow.nodes.should_use_llm_node import should_use_llm_node, route_based_on_confidence
from app.langgraph_flow.nodes.classify_intent_node import classify_intent_node
from app.langgraph_flow.nodes.extract_parameters_node import extract_parameters_node
from app.langgraph_flow.nodes.determine_missing_slot_node import determine_missing_slot_node
from app.langgraph_flow.nodes.ask_next_question_node import ask_next_question_node
from app.langgraph_flow.nodes.refusal_node import refusal_node
from app.langgraph_flow.nodes.flight_search_node import flight_search_node
from app.langgraph_flow.nodes.ranking_node import ranking_node
from app.langgraph_flow.nodes.finalize_response_node import finalize_response_node

# Import new rule-based nodes (to be created separately)
from app.langgraph_flow.nodes.rule_based_question_node import rule_based_question_node
from app.langgraph_flow.nodes.rule_based_ranking_node import rule_based_ranking_node

logger = logging.getLogger(__name__)


# ==========================================
# CONDITIONAL EDGE FUNCTIONS
# ==========================================

def route_after_intent_classification(
    state: ConversationState
) -> Literal["refusal", "extract_parameters"]:
    """
    Route based on intent classification (LLM path only).
    
    If intent is 'irrelevant' or 'chitchat' â†’ refusal_node
    Otherwise â†’ extract_parameters_node
    """
    intent = state.get("intent", "")
    
    if intent in ["irrelevant", "chitchat"]:
        logger.info(f"Intent '{intent}' detected â†’ routing to refusal_node")
        return "refusal"
    
    logger.info(f"Intent '{intent}' detected â†’ routing to extract_parameters")
    return "extract_parameters"


def route_after_missing_slot_check(
    state: ConversationState
) -> Literal["ask_question", "rule_question", "flight_search"]:
    """
    Route based on missing parameters and which path we're on.
    
    If missing_parameter exists:
        - If rule-based path â†’ rule_based_question_node
        - If LLM path â†’ ask_next_question_node (LLM)
    
    If all slots filled â†’ flight_search_node
    """
    missing_param = state.get("missing_parameter")
    use_rule_based = state.get("use_rule_based_path", False)
    
    if missing_param:
        if use_rule_based:
            logger.info(f"Missing parameter: {missing_param} â†’ routing to rule_based_question (NO LLM)")
            return "rule_question"
        else:
            logger.info(f"Missing parameter: {missing_param} â†’ routing to ask_question (LLM)")
            return "ask_question"
    
    logger.info("All slots filled â†’ routing to flight_search")
    return "flight_search"


def route_after_flight_search(
    state: ConversationState
) -> Literal["rule_ranking", "llm_ranking"]:
    """
    Route to appropriate ranking node based on path.
    
    Rule-based path â†’ cheap_ranker (no LLM)
    LLM path â†’ full ranking with explanations
    """
    use_rule_based = state.get("use_rule_based_path", False)
    
    if use_rule_based:
        logger.info("â†’ Routing to rule_based_ranking (cheap_ranker)")
        return "rule_ranking"
    else:
        logger.info("â†’ Routing to llm_ranking")
        return "llm_ranking"


# ==========================================
# GRAPH CONSTRUCTION (HYBRID VERSION)
# ==========================================

def create_trip_planner_graph() -> StateGraph:
    """
    Build the HYBRID LangGraph with rule-based/LLM routing.
    
    Flow:
    1. EntryNode
    2. LoadSessionStateNode
    3. LoadUserMemoryNode
    4. **ShouldUseLLMNode** â† THE KEY ROUTING NODE
    5. Branch:
       
       RULE-BASED PATH (high confidence):
       â†’ DetermineMissingSlotNode
       â†’ RuleBasedQuestionNode (templates) â†’ END
       â†’ OR FlightSearchNode â†’ RuleBasedRankingNode (cheap_ranker) â†’ END
       
       LLM PATH (low confidence):
       â†’ ClassifyIntentNode (LLM)
       â†’ ExtractParametersNode (LLM)
       â†’ DetermineMissingSlotNode
       â†’ AskNextQuestionNode (LLM) â†’ END
       â†’ OR FlightSearchNode â†’ RankingNode (LLM) â†’ END
    
    Expected performance:
    - 80-95% of requests use rule-based path
    - 5-20% fallback to LLM
    - 80-100% cost savings
    - 75% latency reduction
    """
    
    graph = StateGraph(ConversationState)
    
    # ==========================================
    # ADD ALL NODES (including new routing nodes)
    # ==========================================
    
    graph.add_node("entry", entry_node)
    graph.add_node("load_session_state", load_session_state_node)
    graph.add_node("load_user_memory", load_user_memory_node)
    
    # **THE KEY ROUTING NODE**
    graph.add_node("should_use_llm", should_use_llm_node)
    
    # LLM path nodes
    graph.add_node("classify_intent", classify_intent_node)
    graph.add_node("extract_parameters", extract_parameters_node)
    graph.add_node("ask_question", ask_next_question_node)
    graph.add_node("llm_ranking", ranking_node)
    
    # Rule-based path nodes
    graph.add_node("rule_question", rule_based_question_node)
    graph.add_node("rule_ranking", rule_based_ranking_node)
    
    # Shared nodes
    graph.add_node("determine_missing_slot", determine_missing_slot_node)
    graph.add_node("refusal", refusal_node)
    graph.add_node("flight_search", flight_search_node)
    graph.add_node("finalize_response", finalize_response_node)
    
    # ==========================================
    # DEFINE EDGES (HYBRID FLOW)
    # ==========================================
    
    # Set entry point
    graph.set_entry_point("entry")
    
    # Linear flow: Entry â†’ LoadSession â†’ LoadMemory â†’ ShouldUseLLM
    graph.add_edge("entry", "load_session_state")
    graph.add_edge("load_session_state", "load_user_memory")
    graph.add_edge("load_user_memory", "should_use_llm")
    
    # **CRITICAL ROUTING DECISION**
    graph.add_conditional_edges(
        "should_use_llm",
        route_based_on_confidence,
        {
            "rule_based_path": "determine_missing_slot",  # Skip LLM nodes!
            "llm_path": "classify_intent",  # Use LLM nodes
        }
    )
    
    # LLM PATH: classify_intent â†’ refusal OR extract_parameters
    graph.add_conditional_edges(
        "classify_intent",
        route_after_intent_classification,
        {
            "refusal": "refusal",
            "extract_parameters": "extract_parameters",
        }
    )
    
    # LLM PATH: extract_parameters â†’ determine_missing_slot
    graph.add_edge("extract_parameters", "determine_missing_slot")
    
    # Refusal â†’ END
    graph.add_edge("refusal", END)
    
    # After missing slot check: route to appropriate question node or flight search
    graph.add_conditional_edges(
        "determine_missing_slot",
        route_after_missing_slot_check,
        {
            "ask_question": "ask_question",  # LLM question
            "rule_question": "rule_question",  # Template question
            "flight_search": "flight_search",
        }
    )
    
    # Question nodes â†’ END (wait for user)
    graph.add_edge("ask_question", END)
    graph.add_edge("rule_question", END)
    
    # FlightSearch â†’ route to appropriate ranking
    graph.add_conditional_edges(
        "flight_search",
        route_after_flight_search,
        {
            "rule_ranking": "rule_ranking",
            "llm_ranking": "llm_ranking",
        }
    )
    
    # Ranking nodes â†’ finalize â†’ END
    graph.add_edge("rule_ranking", "finalize_response")
    graph.add_edge("llm_ranking", "finalize_response")
    graph.add_edge("finalize_response", END)
    
    logger.info("âœ… HYBRID Trip Planner graph constructed successfully")
    
    return graph


# ==========================================
# COMPILED GRAPH (READY TO USE)
# ==========================================

trip_planner_graph = create_trip_planner_graph().compile()

logger.info("âœ… HYBRID LangGraph flow compiled and ready")
logger.info("ðŸš€ Routing enabled: Rule-based path for 80-95% of requests")


# ==========================================
# GRAPH EXECUTION HELPER
# ==========================================

async def run_conversation_turn(
    session_id: str,
    user_id: Optional[int],
    user_message: str,
) -> ConversationState:
    """
    Execute one turn of the conversation with hybrid routing.
    
    The graph will automatically decide whether to use:
    - Rule-based path (fast, cheap)
    - LLM path (fallback)
    
    Args:
        session_id: Session identifier
        user_id: User ID (None for anonymous)
        user_message: Latest user input
    
    Returns:
        Updated ConversationState after graph execution
    """
    from app.langgraph_flow.state import create_initial_state
    
    initial_state = create_initial_state(
        session_id=session_id,
        user_id=user_id,
        latest_message=user_message,
    )
    
    logger.info(f"ðŸš€ Starting HYBRID conversation turn for session {session_id}")
    
    try:
        final_state = await trip_planner_graph.ainvoke(initial_state)
        
        # Log which path was used
        used_rule_based = final_state.get("use_rule_based_path", False)
        confidence = final_state.get("confidence_breakdown", {}).get("overall_confidence", 0)
        
        if used_rule_based:
            logger.info(f"âœ… Completed via RULE-BASED path (confidence: {confidence:.2f}) - ZERO LLM cost!")
        else:
            logger.info(f"âš ï¸ Completed via LLM path (confidence: {confidence:.2f}) - fallback used")
        
        return final_state
    
    except Exception as e:
        logger.error(f"Error in conversation turn: {e}", exc_info=True)
        raise


# ==========================================
# GRAPH VISUALIZATION
# ==========================================

def visualize_graph() -> str:
    """
    Mermaid diagram of HYBRID graph structure.
    """
    return """
    graph TD
        START([User Message]) --> entry[Entry Node]
        entry --> load_session[Load Session State]
        load_session --> load_memory[Load User Memory]
        load_memory --> should_llm{Should Use LLM?}
        
        should_llm -->|HIGH confidence| determine[Determine Missing Slot]
        should_llm -->|LOW confidence| classify[Classify Intent - LLM]
        
        classify -->|irrelevant| refusal[Refusal]
        classify -->|travel| extract[Extract Parameters - LLM]
        extract --> determine
        
        determine -->|missing + rule path| rule_q[Rule Question - Template]
        determine -->|missing + LLM path| ask_q[Ask Question - LLM]
        determine -->|complete| search[Flight Search]
        
        rule_q --> END1([END])
        ask_q --> END2([END])
        refusal --> END3([END])
        
        search -->|rule path| rule_rank[Rule Ranking - Cheap]
        search -->|LLM path| llm_rank[LLM Ranking]
        
        rule_rank --> finalize[Finalize Response]
        llm_rank --> finalize
        finalize --> END4([END])
        
        classDef ruleClass fill:#4CAF50,stroke:#2E7D32,color:#fff
        classDef llmClass fill:#FF9800,stroke:#E65100,color:#fff
        classDef routingClass fill:#2196F3,stroke:#0D47A1,color:#fff
        
        class rule_q,rule_rank ruleClass
        class classify,extract,ask_q,llm_rank llmClass
        class should_llm routingClass
    """


if __name__ == "__main__":
    print("=" * 80)
    print("AI TRIP PLANNER - HYBRID LANGGRAPH FLOW")
    print("=" * 80)
    print(visualize_graph())
    print("=" * 80)